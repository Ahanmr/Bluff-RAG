# CALM-RAG500
We propose CALM‑RAG‑500, a benchmark that evaluates not only factual correctness of Retrieval‑Augmented Generation (RAG) systems but also how well models calibrate their confidence and express appropriate linguistic uncertainty.  Each of 500 question‑answer pairs is paired with partially conflicting passages. 
